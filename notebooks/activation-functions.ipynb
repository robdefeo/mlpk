{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "\n",
    "**What do activation layers actually do?** TODO:\n",
    "They get the result in to the format required for the next layer, e.g. in the\n",
    "case the result needs to be a one-hot encoding, the function will be applied to\n",
    "each of the values to make the results as close as possible to 1 or 0 for easy\n",
    "conversion. The easier it is for the neural net to create the thing we want the\n",
    "fast and more accurate the result will be.\n",
    "\n",
    "\n",
    "**What is soft max activation function?** TODO:\n",
    "It converts the output values so they are as close as possible to 1 or 0, e.g.\n",
    "one-hot encoded, ideal for use in multi-class classification problems.\n",
    "\n",
    "\n",
    "**What activation functions should I use where?** TODO:\n",
    "If in doubt use \"relu\" everywhere apart form the last layer and there use\n",
    "\"softmax\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
