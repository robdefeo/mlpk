{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is TSNE?\n",
    "It is a way of compressing a large vector so the similar things can be plotted on a chart to show similar items next to each other. E.g. create the cool charts that plots similar words next to each other, a 2d representation is simple so something things will appear strangely.\n",
    "\n",
    "What is the deep visualization tool box?\n",
    "Upload image and select layer and visualize each of the filters\n",
    "\n",
    "\n",
    "\n",
    "If a neural network can theoretically approximate anything why does model architecture and activations even matter?\n",
    "It would take an impossibly long time to complete a satisfactory calculation.\n",
    "\n",
    "The goal of machine learning is to produce models that generalize to previously unseen data. !!!!\n",
    "\n",
    "What is multi-class classification?\n",
    "\n",
    "Select the most appropriate class based on 2+ options, results in a single answer of a class to a problem. E.g. is a Plane\n",
    "\n",
    "\n",
    "\n",
    "What is an example of a loss function?\n",
    "Root mean squared.\n",
    "\n",
    "What does root squared mean loss function show?\n",
    "How far out on average are the predictions.\n",
    "\n",
    "What is feature engineering and why is this different in deep learning?\n",
    "\n",
    "\n",
    "What resources do dense layers need for training?\n",
    "\n",
    "Memory there are a lot of weights that need to be loaded.\n",
    "\n",
    "\n",
    "What resources do convolution layers need for training?\n",
    "\n",
    "Huge amounts of compute as there are many dimensions to calculate and it therefore takes a lot of time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is underfitting?\n",
    "\n",
    "A model that does not have enough parameters its is not complex enough to solve the task you are trying.\n",
    "\n",
    "\n",
    "Is overfitting really a bad thing?\n",
    "\n",
    "It's an indication…. If depends on how much you are over fitting but it does probably mean that the model is complex enough and should start using over fitting strategies, drop off, data augmentation etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do I know if I have over fitted?\n",
    "\n",
    "Training set will have a much higher accuracy than test or validation set.\n",
    "\n",
    "\n",
    "How do I know if I have underfitted?\n",
    "\n",
    "Validation accuracy is higher than the training accuracy\n",
    "\n",
    "\n",
    "\n",
    "Deeper layers have smaller filters but they cover a larger area\n",
    "\n",
    "\n",
    "What is a dropout layer? / What is drop off?\n",
    "Dropout layers are added at the end of a FullyConnectedBlock and delete 50% of the activations at random, the reason is when random parts of the network are thrown away the network cant learn to overfit. It mean big complex models can be trained for long periods of time without overfitting.\n",
    "\n",
    "\n",
    "How much drop out should I use?\n",
    "Typically it increases e.g. 10%, 20%, 30%, 40%, 50% max (50%) at each layer, if underfitting the values can be modified lower. If you drop out too much in the early layers that information is lost to all successive layers.\n",
    "\n",
    "\n",
    "What is data augmentation?\n",
    "Take an image and tilt it by varying degrees, rotate, flip, move up and down, zoom in, zoom out, keras has a ImageDataGenerator that creates these at random.\n",
    "\n",
    "\n",
    "Should I always use data augmentation?\n",
    "Yes, there is no reason not to, better question is what type and how much, which is more difficult to answer.\n",
    "\n",
    "\n",
    "What is regularization?\n",
    "\n",
    "\n",
    "How much regularization do you need?\n",
    "Regularization relates to data volume, more data you have less regularization you need.\n",
    "\n",
    "\n",
    "Should I shuffle or augment the data validation set?\n",
    "\n",
    "No, it must stay static this is what you are verifying against if it changes there will be confusing problems.\n",
    "\n",
    "\n",
    "What is batch normalisation?\n",
    "\n",
    "If the input weights have large differences between them, small changes can have a large or small effect on the model, the further down the layers the larger this effect. This makes model more unpredicatble and difficult to train, normalisation helps to resolve this, by bringing the values into a range.\n",
    "\n",
    "\n",
    "What is batchnorm?\n",
    "\n",
    "It normalises the intermediate layers values with a different method but similar end results to how input values are normalised.\n",
    "\n",
    "\n",
    "What is ensembling?\n",
    "\n",
    "Build multiple version of model and combine them together, train X models from different starting points they will have different errors, so combine the averages to get a high accuracy.\n",
    "\n",
    "\n",
    "What is fine tuning actually doing behind the scenes?\n",
    "\n",
    "Remove some of the later layers, and retrain them from scratch.\n",
    "\n",
    "E.g. model.pop()\n",
    "\n",
    "For layer in model.layers: layer.trainable = False\n",
    "\n",
    "Making the exsiting lower layers untrainable means it wont update their values\n",
    "\n",
    "Then add the layer need for specific training \n",
    "\n",
    "Model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "Then fit model\n",
    "\n",
    "\n",
    "How many layers should removed in fine tuning?\n",
    "\n",
    "Depends on how different the problem is to the existing model, the more different the more layers will need to be retrained.\n",
    "\n",
    "\n",
    "What is knowledge distilation?\n",
    "\n",
    "\n",
    "What is psudeo labeling?\n",
    "\n",
    "\n",
    "What is semi supervised learning?\n",
    "\n",
    "Its using the unlabled (test data) to understand the structure of the data, e.g. use test set without labels, have model predit label then added data and predicted label to the training set. If useful cus it can learn the features.\n",
    "\n",
    "\n",
    "After improving the model should I re-do the psudeo labeling?\n",
    "\n",
    "Yes\n",
    "\n",
    "\n",
    "Is there too much psuedo labling that can be done?\n",
    "\n",
    "Yes, use approx 25-33% max for it.\n",
    "\n",
    "\n",
    "\n",
    "WHAt is DSSM?\n",
    "\n",
    "Look up the mxnet demo on it\n",
    "\n",
    "\n",
    "What is collaborative filtering?\n",
    "\n",
    "It’s a way of doing reccomender systems ,instead of trying to look at meta data and recommend, do it from user behaviour, e.g. this user like x so similar use will like it too. Features are created at random then optimsed instead of pre-specifying what they could be.\n",
    "\n",
    "\n",
    "What is an attentional model?\n",
    "\n",
    "Attempt as simulating an eye, centre of image is in focus, when the thing you are looking for is not in the middle and in low resolution part of the image then it will be refocused to have this in the centre. \n",
    "\n",
    "\n",
    "What is learning rate?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What happens if the learning rate is too low? \n",
    "\n",
    "It will take much longer to find the answer, ideally the learning rate should allow the step to be either side of the line.\n",
    "\n",
    "\n",
    "What happens if the learning rate is too high?\n",
    "\n",
    "The steps become much bigger and it gets further and further from the answer.\n",
    "\n",
    "\n",
    "What is dynamic learning rates?\n",
    "\n",
    "Different parameters can be set with different learning rates that are appropriate for them individually. It would mean that each parameter in a filter would have its own learning rate, e.g. 3x3 would have 9, for each filter in each layer.\n",
    "\n",
    "\n",
    "What special considerations should be taken for learning rates of new model?\n",
    "\n",
    "A new model might just pick a class and set everything as that, e.g. always say 8, in this case set the learning rate very low e.g. '1e-5' have the validation score come out as significantly better than random then set learning rate as per the normal approach.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If you started off with a large learning weight for speed then you would need to make it smaller as you got closer to the lower error rate, otherwise the steps would be too large.As your validation scores flatten out then try dividing learning rate by 10 and try again, repeat. \n",
    "\n",
    "\n",
    "What is momentum?\n",
    "\n",
    "The step is going either side of the line to form saddle points an average can be used to get it to fit faster, take an average of the last few steps to calculate the relevant part of the line. Momentum parameters are something that you need to pick just like learning rate, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How to represent text in NN?\n",
    "\n",
    "LSTM, Bag of words\n",
    "\n",
    "\n",
    "\n",
    "How to image text in NN?\n",
    "\n",
    "\n",
    "\n",
    "What is a reccurent neural network?\n",
    "\n",
    "\n",
    "\n",
    "What is negative sampling?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is cosinLoss layer?\n",
    "\n",
    "\n",
    "\n",
    "What is spare data?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Activation functions: After as single matrix multiple an activation is created which is the result of the matrix multiple, the activation function is the action that is applied to that result, this is a non linear function. Common functions are tanh, sigmod, relu\n",
    "\n",
    "\n",
    "\n",
    "Deep learning is a matrix multiply at multiple levels because though this can be expressed as a single matrix multiple there is an additonal step at after each matrix multiply\n",
    "\n",
    "Linear algebra\n",
    "\n",
    "\n",
    "\n",
    "Layer\n",
    "\n",
    "\n",
    "\n",
    "Neural network\n",
    "\n",
    "\n",
    "\n",
    "Sequencial: is an array that takes all the layers, allows building multi layer\n",
    "\n",
    "\n",
    "Stochastic gradient descent: is an optimisation prodecure.The most important thing in ML starts with random weights and ends with weights that you want.\n",
    "\n",
    "\n",
    "\n",
    "Weights\n",
    "\n",
    "\n",
    "\n",
    "Linear model: is a single layer model\n",
    "\n",
    "\n",
    "\n",
    "Dense Layer: Is a linear model or fully connected\n",
    "\n",
    "\n",
    "\n",
    "One hot encoding: An array of bits with one value being true\n",
    "\n",
    "\n",
    "\n",
    "RMS Prop:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
