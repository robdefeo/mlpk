{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is a good process for training a machine learning model important?** TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is training data set?**  \n",
    "The data that will to train and adjust the wieghts of the model, exists in the `./train` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a validation data set?**  \n",
    "A smaller set of data used to prevent overfitting this data does not adjust the weights of the network. It will verify that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're [overfitting](overfitting.md) your neural network and you should stop training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do I do if a validation data set does not exist?**  \n",
    "Maintain the same structure and lables as the train dataset, move a percentage (10% is typical but depends on training set size) of the data to a `./valid` folder. **Note** Move the data from training do not copy it, otherwise the model will validate against data learnt from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a test set?**  \n",
    "This data set for testing the final solution to confirm the actual predictive power of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is test data important?**  \n",
    "Without test data there is no way to evaluate how the model will actually perform, for example, a trivial algorithm that memorizes its inputs and stores the associated labels. This model would have 100% accuracy on training data but would have no way of making any prediction at all on previously unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to do if a test set does not exist?**  \n",
    "Split the data up (typical 20% for test) first of all and move it to a `./test` folder. **Note** Move the data from training do not copy it, otherwise the model will have learnt from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is sample data?**  \n",
    "A subset of the train / valid data sets that used to iterate model design or parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is sample data important?**  \n",
    "It's a small amount of data, meaning that all experiments run quickly to get fast results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**How do I create sample data if it does not exist?**  \n",
    "Maintian the same structure and labels as the test / valid data sets but move a percentage of the data (typically 1% but depends on data set size) to a `sample` folder, e.g. there will be a `./sample/test` and `./sample/valid` folders.  **Note** Copy ths data rather than moving it as it is ok for the data to exist in two places as you dont want to loose any data when training against the full dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
