{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a loss function?**  \n",
    "A way measuring model performance or answering the question \"how bad is our model?\", using a loss function the model can determine the effect increasing or decreasing model parameters. It does this by comparing the output of the model with the truth. E.g. Loss functions allow us to determine whether a stock prediction of $1,500 for AMZN by December 31, 2017 is correct. Depending on whether we decided to go short or long on it, we would incur different losses (or realize profits), hence our loss functions might be quite different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is a loss function important?**  \n",
    "Train a model means making it better and better over the course of a period of training. But in order for this goal to make any sense at all, we first need to define what better means in the first place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is cross-entropy loss function?**  \n",
    "Used widely in deep learning, it is appropiate for (multi) classification problems. TODO: GOT TIRED!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "TODO:\n",
    "In machine learning, we train models to get better and better as a function of experience. Usually, getting better means minimizing a loss function, i.e. a score that answers \"how bad is our model?\" With neural networks, we choose loss functions to be differentiable with respect to our parameters.\n",
    "\n",
    "MXNet's autograd package expedites this work by automatically calculating derivatives. And while most other libraries require that we compile a symbolic graph to take automatic derivatives, mxnet.autograd, like PyTorch, allows you to take derivatives while writing ordinary imperative code.\n",
    "Every time you make pass through your model, autograd builds a graph on the fly, through which it can immediately backpropagate gradients.\n",
    "Let's go through it step by step. For this tutorial, we'll only need to import mxnet.ndarray, and mxnet.autograd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
